[k8s.io] Cluster size autoscaler scalability [Slow] CA ignores unschedulable pods while scheduling schedulable pods [Feature:ClusterAutoscalerScalability6]
[k8s.io] Cluster size autoscaler scalability [Slow] should scale down empty nodes [Feature:ClusterAutoscalerScalability3]
[k8s.io] Cluster size autoscaler scalability [Slow] should scale down underutilized nodes [Feature:ClusterAutoscalerScalability4]
[k8s.io] Cluster size autoscaler scalability [Slow] should scale up at all [Feature:ClusterAutoscalerScalability1]
[k8s.io] Cluster size autoscaler scalability [Slow] should scale up twice [Feature:ClusterAutoscalerScalability2]
[k8s.io] Cluster size autoscaler scalability [Slow] shouldn't scale down with underutilized nodes due to host port conflicts [Feature:ClusterAutoscalerScalability5]
[k8s.io] GKE local SSD [Feature:GKELocalSSD] should write and read from node local SSD [Feature:GKELocalSSD]
[k8s.io] GKE node pools [Feature:GKENodePool] should create a cluster with multiple node pools [Feature:GKENodePool]
[k8s.io] Probing container should be restarted with a docker exec liveness probe with timeout
[k8s.io] [sig-node] AppArmor load AppArmor profiles can disable an AppArmor profile, using unconfined
[k8s.io] [sig-node] AppArmor load AppArmor profiles should enforce an AppArmor profile
[k8s.io] [sig-node] SSH should SSH to all nodes and run commands
[k8s.io] [sig-node] crictl should be able to run crictl on the node
[sig-api-machinery] AdmissionWebhook Should be able to deny attaching pod
[sig-api-machinery] AdmissionWebhook Should be able to deny custom resource creation
[sig-api-machinery] AdmissionWebhook Should be able to deny pod and configmap creation
[sig-api-machinery] AdmissionWebhook Should deny crd creation
[sig-api-machinery] AdmissionWebhook Should mutate configmap
[sig-api-machinery] AdmissionWebhook Should mutate custom resource
[sig-api-machinery] AdmissionWebhook Should mutate pod and apply defaults after mutation
[sig-api-machinery] AdmissionWebhook Should not be able to mutate or prevent deletion of webhook configuration objects
[sig-api-machinery] AdmissionWebhook Should unconditionally reject operations on fail closed webhook
[sig-api-machinery] Aggregator Should be able to support the 1.10 Sample API Server using the current Aggregator
[sig-apps] ReplicaSet should serve a basic image on each replica with a private image
[sig-apps] ReplicationController should serve a basic image on each replica with a private image
[sig-auth] Advanced Audit should audit API calls [DisabledForLargeClusters]
[sig-auth] Metadata Concealment should run a check-metadata-concealment job to completion
[sig-autoscaling] Cluster size autoscaling [Slow] Should be able to scale a node group down to 0[Feature:ClusterSizeAutoscalingScaleDown]
[sig-autoscaling] Cluster size autoscaling [Slow] Should be able to scale a node group up from 0[Feature:ClusterSizeAutoscalingScaleUp]
[sig-autoscaling] Cluster size autoscaling [Slow] Should not scale GPU pool up if pod does not require GPUs [GpuType:] [Feature:ClusterSizeAutoscalingGpu]
[sig-autoscaling] Cluster size autoscaling [Slow] Should scale down GPU pool from 1 [GpuType:] [Feature:ClusterSizeAutoscalingGpu]
[sig-autoscaling] Cluster size autoscaling [Slow] Should scale up GPU pool from 0 [GpuType:] [Feature:ClusterSizeAutoscalingGpu]
[sig-autoscaling] Cluster size autoscaling [Slow] Should scale up GPU pool from 1 [GpuType:] [Feature:ClusterSizeAutoscalingGpu]
[sig-autoscaling] Cluster size autoscaling [Slow] Shouldn't perform scale up operation and should list unhealthy status if most of the cluster is broken[Feature:ClusterSizeAutoscalingScaleUp]
[sig-autoscaling] Cluster size autoscaling [Slow] should add node to the particular mig [Feature:ClusterSizeAutoscalingScaleUp]
[sig-autoscaling] Cluster size autoscaling [Slow] should be able to scale down by draining multiple pods one by one as dictated by pdb[Feature:ClusterSizeAutoscalingScaleDown]
[sig-autoscaling] Cluster size autoscaling [Slow] should be able to scale down by draining system pods with pdb[Feature:ClusterSizeAutoscalingScaleDown]
[sig-autoscaling] Cluster size autoscaling [Slow] should be able to scale down when rescheduling a pod is required and pdb allows for it[Feature:ClusterSizeAutoscalingScaleDown]
[sig-autoscaling] Cluster size autoscaling [Slow] should correctly scale down after a node is not needed [Feature:ClusterSizeAutoscalingScaleDown]
[sig-autoscaling] Cluster size autoscaling [Slow] should correctly scale down after a node is not needed and one node is broken [Feature:ClusterSizeAutoscalingScaleDown]
[sig-autoscaling] Cluster size autoscaling [Slow] should correctly scale down after a node is not needed when there is non autoscaled pool[Feature:ClusterSizeAutoscalingScaleDown]
[sig-autoscaling] Cluster size autoscaling [Slow] should disable node pool autoscaling [Feature:ClusterSizeAutoscalingScaleUp]
[sig-autoscaling] Cluster size autoscaling [Slow] should increase cluster size if pending pods are small [Feature:ClusterSizeAutoscalingScaleUp]
[sig-autoscaling] Cluster size autoscaling [Slow] should increase cluster size if pending pods are small and one node is broken [Feature:ClusterSizeAutoscalingScaleUp]
[sig-autoscaling] Cluster size autoscaling [Slow] should increase cluster size if pending pods are small and there is another node pool that is not autoscaled [Feature:ClusterSizeAutoscalingScaleUp]
[sig-autoscaling] Cluster size autoscaling [Slow] should increase cluster size if pod requesting EmptyDir volume is pending [Feature:ClusterSizeAutoscalingScaleUp]
[sig-autoscaling] Cluster size autoscaling [Slow] should increase cluster size if pod requesting volume is pending [Feature:ClusterSizeAutoscalingScaleUp]
[sig-autoscaling] Cluster size autoscaling [Slow] should increase cluster size if pods are pending due to host port conflict [Feature:ClusterSizeAutoscalingScaleUp]
[sig-autoscaling] Cluster size autoscaling [Slow] should increase cluster size if pods are pending due to pod anti-affinity [Feature:ClusterSizeAutoscalingScaleUp]
[sig-autoscaling] Cluster size autoscaling [Slow] should scale down when expendable pod is running [Feature:ClusterSizeAutoscalingScaleDown]
[sig-autoscaling] Cluster size autoscaling [Slow] should scale up correct target pool [Feature:ClusterSizeAutoscalingScaleUp]
[sig-autoscaling] Cluster size autoscaling [Slow] should scale up when non expendable pod is created [Feature:ClusterSizeAutoscalingScaleUp]
[sig-autoscaling] Cluster size autoscaling [Slow] shouldn't be able to scale down when rescheduling a pod is required, but pdb doesn't allow drain[Feature:ClusterSizeAutoscalingScaleDown]
[sig-autoscaling] Cluster size autoscaling [Slow] shouldn't increase cluster size if pending pod is too large [Feature:ClusterSizeAutoscalingScaleUp]
[sig-autoscaling] Cluster size autoscaling [Slow] shouldn't scale down when non expendable pod is running [Feature:ClusterSizeAutoscalingScaleDown]
[sig-autoscaling] Cluster size autoscaling [Slow] shouldn't scale up when expendable pod is created [Feature:ClusterSizeAutoscalingScaleUp]
[sig-autoscaling] Cluster size autoscaling [Slow] shouldn't scale up when expendable pod is preempted [Feature:ClusterSizeAutoscalingScaleUp]
[sig-autoscaling] Cluster size autoscaling [Slow] shouldn't trigger additional scale-ups during processing scale-up [Feature:ClusterSizeAutoscalingScaleUp]
[sig-autoscaling] DNS horizontal autoscaling [DisabledForLargeClusters] kube-dns-autoscaler should scale kube-dns pods in both nonfaulty and faulty scenarios
[sig-autoscaling] DNS horizontal autoscaling [Serial] [Slow] kube-dns-autoscaler should scale kube-dns pods when cluster size changed
[sig-autoscaling] [Feature:ClusterSizeAutoscalingScaleUp] [Slow] Autoscaling [sig-autoscaling] Autoscaling a service from 1 pod and 3 nodes to 8 pods and &gt;
[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: Custom Metrics from Stackdriver) should scale down with Custom Metric of type Object from Stackdriver [Feature:CustomMetricsAutoscaling]
[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: Custom Metrics from Stackdriver) should scale down with Custom Metric of type Pod from Stackdriver [Feature:CustomMetricsAutoscaling]
[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: Custom Metrics from Stackdriver) should scale down with Custom Metric of type Pod from Stackdriver with Prometheus [Feature:CustomMetricsAutoscaling]
[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: Custom Metrics from Stackdriver) should scale down with External Metric with target average value from Stackdriver [Feature:CustomMetricsAutoscaling]
[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: Custom Metrics from Stackdriver) should scale down with External Metric with target value from Stackdriver [Feature:CustomMetricsAutoscaling]
[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: Custom Metrics from Stackdriver) should scale up with two External metrics from Stackdriver [Feature:CustomMetricsAutoscaling]
[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: Custom Metrics from Stackdriver) should scale up with two metrics of type Pod from Stackdriver [Feature:CustomMetricsAutoscaling]
[sig-cli] Kubectl alpha client [k8s.io] Kubectl run CronJob should create a CronJob
[sig-cluster-lifecycle] Addon update should propagate add-on file changes [Slow]
[sig-cluster-lifecycle] kube-proxy migration [Feature:KubeProxyDaemonSetMigration] Downgrade kube-proxy from a DaemonSet to static pods should maintain a functioning cluster [Feature:KubeProxyDaemonSetDowngrade]
[sig-cluster-lifecycle] kube-proxy migration [Feature:KubeProxyDaemonSetMigration] Upgrade kube-proxy from static pods to a DaemonSet should maintain a functioning cluster [Feature:KubeProxyDaemonSetUpgrade]
[sig-instrumentation] Cluster level logging implemented by Stackdriver should ingest events [Feature:StackdriverLogging]
[sig-instrumentation] Cluster level logging implemented by Stackdriver should ingest logs [Feature:StackdriverLogging]
[sig-instrumentation] Cluster level logging implemented by Stackdriver should ingest system logs from all nodes [Feature:StackdriverLogging]
[sig-instrumentation] Cluster level logging using Elasticsearch [Feature:Elasticsearch] should check that logs from containers are ingested into Elasticsearch
[sig-instrumentation] Kibana Logging Instances Is Alive [Feature:Elasticsearch] should check that the Kibana logging instance is alive
[sig-instrumentation] Stackdriver Monitoring should have accelerator metrics [Feature:StackdriverAcceleratorMonitoring]
[sig-instrumentation] Stackdriver Monitoring should have cluster metrics [Feature:StackdriverMonitoring]
[sig-instrumentation] Stackdriver Monitoring should run Custom Metrics - Stackdriver Adapter for external metrics [Feature:StackdriverExternalMetrics]
[sig-instrumentation] Stackdriver Monitoring should run Custom Metrics - Stackdriver Adapter for new resource model [Feature:StackdriverCustomMetrics]
[sig-instrumentation] Stackdriver Monitoring should run Custom Metrics - Stackdriver Adapter for old resource model [Feature:StackdriverCustomMetrics]
[sig-instrumentation] Stackdriver Monitoring should run Stackdriver Metadata Agent [Feature:StackdriverMetadataAgent]
[sig-instrumentation] [Feature:PrometheusMonitoring] Prometheus should contain correct container CPU metric.
[sig-instrumentation] [Feature:PrometheusMonitoring] Prometheus should scrape container metrics from all nodes.
[sig-instrumentation] [Feature:PrometheusMonitoring] Prometheus should scrape metrics from annotated pods.
[sig-instrumentation] [Feature:PrometheusMonitoring] Prometheus should scrape metrics from annotated services.
[sig-instrumentation] [Feature:PrometheusMonitoring] Prometheus should successfully scrape all targets
[sig-network] ESIPP [Slow] [DisabledForLargeClusters] should handle updates to ExternalTrafficPolicy field
[sig-network] ESIPP [Slow] [DisabledForLargeClusters] should only target nodes with endpoints
[sig-network] ESIPP [Slow] [DisabledForLargeClusters] should work for type
[sig-network] ESIPP [Slow] [DisabledForLargeClusters] should work from pods
[sig-network] Firewall rule [Slow] [Serial] should create valid firewall rules for LoadBalancer type service
[sig-network] Firewall rule should have correct firewall rules for e2e cluster
[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:Ingress] multicluster ingress should get instance group annotation
[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:Ingress] should be able to switch between HTTPS and HTTP2 modes
[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:Ingress] should conform to Ingress spec
[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:Ingress] should create ingress with backend HTTPS
[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:Ingress] should create ingress with given static-ip
[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:Ingress] should create ingress with pre-shared certificate
[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:Ingress] should not reconcile manually modified health check for ingress
[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:Ingress] should support multiple TLS certs
[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:Ingress] should update ingress while sync failures occur on other ingresses
[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:NEG] rolling update backend pods should not cause service disruption
[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:NEG] should be able to create a ClusterIP service
[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:NEG] should be able to switch between IG and NEG modes
[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:NEG] should conform to Ingress spec
[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:NEG] should create NEGs for all ports with the Ingress annotation, and NEGs for the standalone annotation otherwise
[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:NEG] should sync endpoints for both Ingress-referenced NEG and standalone NEG
[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:NEG] should sync endpoints to NEG
[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:kubemci] should conform to Ingress spec
[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:kubemci] should create ingress with backend HTTPS
[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:kubemci] should create ingress with pre-shared certificate
[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:kubemci] should remove clusters as expected
[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:kubemci] should support https-only annotation
[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:kubemci] single and multi-cluster ingresses should be able to exist together
[sig-network] Loadbalancing: L7 Scalability GCE [Slow] [Serial] [Feature:IngressScale] Creating and updating ingresses should happen promptly with small/medium/large amount of ingresses
[sig-network] Loadbalancing: L7 [Slow] Nginx should conform to Ingress spec
[sig-network] Networking Granular Checks: Services [Slow] should function for client IP based session affinity: http
[sig-network] Networking Granular Checks: Services [Slow] should function for client IP based session affinity: udp
[sig-network] Networking Granular Checks: Services [Slow] should function for endpoint-Service: http
[sig-network] Networking Granular Checks: Services [Slow] should function for endpoint-Service: udp
[sig-network] Networking Granular Checks: Services [Slow] should function for node-Service: http
[sig-network] Networking Granular Checks: Services [Slow] should function for node-Service: udp
[sig-network] Networking Granular Checks: Services [Slow] should function for pod-Service: http
[sig-network] Networking Granular Checks: Services [Slow] should function for pod-Service: udp
[sig-network] Networking Granular Checks: Services [Slow] should update endpoints: http
[sig-network] Networking Granular Checks: Services [Slow] should update endpoints: udp
[sig-network] Networking Granular Checks: Services [Slow] should update nodePort: http [Slow]
[sig-network] Networking Granular Checks: Services [Slow] should update nodePort: udp [Slow]
[sig-network] Networking should check kube-proxy urls
[sig-network] Services [Feature:GCEAlphaFeature][Slow] should be able to create and tear down a standard-tier load balancer [Slow]
[sig-network] Services should be able to change the type and ports of a service [Slow] [DisabledForLargeClusters]
[sig-network] Services should be able to create an internal type load balancer [Slow] [DisabledForLargeClusters]
[sig-network] Services should be able to up and down services
[sig-network] Services should only allow access from service loadbalancer source ranges [Slow]
[sig-network] Services should reconcile LB health check interval [Slow][Serial]
[sig-scheduling] Multi-AZ Cluster Volumes [sig-storage] should only be allowed to provision PDs in zones where nodes exist
[sig-scheduling] Multi-AZ Cluster Volumes [sig-storage] should schedule pods in the same zones as statically provisioned PVs
[sig-scheduling] Multi-AZ Clusters should spread the pods of a replication controller across zones
[sig-scheduling] Multi-AZ Clusters should spread the pods of a service across zones
[sig-apps] stateful Upgrade [Feature:StatefulUpgrade] [k8s.io] stateful upgrade should maintain a functioning cluster
[sig-storage] Dynamic Provisioning DynamicProvisioner Default should create and delete default persistent volumes [Slow]
[sig-storage] Dynamic Provisioning DynamicProvisioner [Slow] deletion should be idempotent
[sig-storage] Dynamic Provisioning DynamicProvisioner [Slow] should not provision a volume in an unmanaged GCE zone.
[sig-storage] Dynamic Provisioning DynamicProvisioner [Slow] should provision storage with non-default reclaim policy Retain
[sig-storage] Dynamic Provisioning DynamicProvisioner [Slow] should test that deleting a claim before the volume is provisioned deletes the volume.
[sig-storage] Dynamic Provisioning Invalid AWS KMS key should report an error and create no PV
[sig-storage] Flexvolumes should be mountable when attachable
[sig-storage] Flexvolumes should be mountable when non-attachable
[sig-storage] GCP Volumes GlusterFS should be mountable
[sig-storage] GCP Volumes NFSv3 should be mountable for NFSv3
[sig-storage] GCP Volumes NFSv4 should be mountable for NFSv4
[sig-storage] Mounted flexvolume expand[Slow] Should verify mounted flex volumes can be resized
[sig-storage] Mounted flexvolume volume expand [Slow] [Feature:ExpandInUsePersistentVolumes] should be resizable when mounted
[sig-storage] Mounted volume expand[Slow] Should verify mounted devices can be resized
[sig-storage] PersistentVolumes Default StorageClass pods that use multiple volumes should be reschedulable [Slow]
[sig-storage] PersistentVolumes GCEPD should test that deleting a PVC before the pod does not cause pod deletion to fail on PD detach
[sig-storage] PersistentVolumes GCEPD should test that deleting the Namespace of a PVC and Pod causes the successful detach of Persistent Disk
[sig-storage] PersistentVolumes GCEPD should test that deleting the PV before the pod does not cause pod deletion to fail on PD detach
[sig-storage] PersistentVolumes [Feature:LabelSelector] [sig-storage] Selector-Label Volume Binding:vsphere should bind volume with claim for given label
[sig-storage] PersistentVolumes [Feature:ReclaimPolicy] [sig-storage] persistentvolumereclaim:vsphere should delete persistent volume when reclaimPolicy set to delete and associated claim is deleted
[sig-storage] PersistentVolumes [Feature:ReclaimPolicy] [sig-storage] persistentvolumereclaim:vsphere should not detach and unmount PV when associated pvc with delete as reclaimPolicy is deleted when it is in use by the pod
[sig-storage] PersistentVolumes [Feature:ReclaimPolicy] [sig-storage] persistentvolumereclaim:vsphere should retain persistent volume when reclaimPolicy set to retain when associated claim is deleted
[sig-storage] Volume FStype [Feature:vsphere] verify invalid fstype
[sig-storage] Volume FStype [Feature:vsphere] verify fstype - default value should be ext4
[sig-storage] Volume Disk Size [Feature:vsphere] verify dynamically provisioned pv using storageclass with an invalid disk size fails
[sig-storage] Volume Disk Format [Feature:vsphere] verify disk format type - zeroedthick is honored for dynamically provisioned pv using storageclass
[sig-storage] Volume Disk Format [Feature:vsphere] verify disk format type - thin is honored for dynamically provisioned pv using storageclass
[sig-storage] Volume Disk Format [Feature:vsphere] verify disk format type - eagerzeroedthick is honored for dynamically provisioned pv using storageclass
[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify if a non-existing SPBM policy is not honored for dynamically provisioned pvc using storageclass
[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify if a SPBM policy is not honored on a non-compatible datastore for dynamically provisioned pvc using storageclass
[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify clean up of stale dummy VM for dynamically provisioned pvc using SPBM policy
[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify an if a SPBM policy and VSAN capabilities cannot be honored for dynamically provisioned pvc using storageclass
[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify an existing and compatible SPBM policy is honored for dynamically provisioned pvc using storageclass
[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify VSAN storage capability with valid objectSpaceReservation and iopsLimit values is honored for dynamically provisioned pvc using storageclass
[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify VSAN storage capability with valid hostFailuresToTolerate and cacheReservation values is honored for dynamically provisioned pvc using storageclass
[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify VSAN storage capability with valid diskStripes and objectSpaceReservation values is honored for dynamically provisioned pvc using storageclass
[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify VSAN storage capability with valid diskStripes and objectSpaceReservation values and a VSAN datastore is honored for dynamically provisioned pvc using storageclass
[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify VSAN storage capability with non-vsan datastore is not honored for dynamically provisioned pvc using storageclass
[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify VSAN storage capability with invalid hostFailuresToTolerate value is not honored for dynamically provisioned pvc using storageclass
[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify VSAN storage capability with invalid diskStripes value is not honored for dynamically provisioned pvc using storageclass
[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify VSAN storage capability with invalid capability name objectSpaceReserve is not honored for dynamically provisioned pvc using storageclass
[sig-storage] Regional PD RegionalPD should provision storage with delayed binding [Slow]
[sig-storage] Regional PD RegionalPD should provision storage in the allowedTopologies with delayed binding [Slow]
[sig-storage] Regional PD RegionalPD should provision storage in the allowedTopologies [Slow]
[sig-storage] Regional PD RegionalPD should provision storage [Slow]
[sig-storage] PersistentVolumes:vsphere should test that deleting the PV before the pod does not cause pod deletion to fail on vspehre volume detach
[sig-storage] PersistentVolumes:vsphere should test that deleting the Namespace of a PVC and Pod causes the successful detach of vsphere volume
[sig-storage] PersistentVolumes:vsphere should test that deleting a PVC before the pod does not cause pod deletion to fail on vsphere volume detach
[sig-storage] vsphere cloud provider stress [Feature:vsphere] vsphere stress tests
[sig-storage] vcp-performance [Feature:vsphere] vcp performance tests
[sig-storage] vcp at scale [Feature:vsphere]  vsphere scale tests
[sig-storage] [Serial] Volume metrics should create volume metrics with the correct PVC ref
[sig-storage] [Serial] Volume metrics should create volume metrics in Volume Manager
[sig-storage] [Serial] Volume metrics should create prometheus metrics for volume provisioning and attach/detach
[sig-storage] [Serial] Volume metrics should create metrics for total time taken in volume operations in P/V Controller
[sig-storage] [Serial] Volume metrics should create metrics for total number of volumes in A/D Controller
[sig-storage] [Serial] Volume metrics PVController should create unbound pvc count metrics for pvc controller after creating pvc only
[sig-storage] [Serial] Volume metrics PVController should create unbound pv count metrics for pvc controller after creating pv only
[sig-storage] [Serial] Volume metrics PVController should create none metrics for pvc controller before creating any PV or PVC
[sig-storage] [Serial] Volume metrics PVController should create bound pv/pvc count metrics for pvc controller after creating both pv and pvc
[sig-storage] Volume expand [Slow] Verify if editing PVC allows resize
[sig-storage] Volume Provisioning on Datastore [Feature:vsphere] verify dynamically provisioned pv using storageclass fails on an invalid datastore
[sig-storage] Volume Provisioning On Clustered Datastore [Feature:vsphere] verify static provisioning on clustered datastore
[sig-storage] Volume Provisioning On Clustered Datastore [Feature:vsphere] verify dynamic provision with spbm policy on clustered datastore
[sig-storage] Volume Provisioning On Clustered Datastore [Feature:vsphere] verify dynamic provision with default parameter on clustered datastore
[sig-storage] Volume Placement test back to back pod creation and deletion with different volume sources on the same worker node
[sig-storage] Volume Placement should create and delete pod with the same volume source on the same worker node
[sig-storage] Volume Placement should create and delete pod with the same volume source attach/detach to different worker nodes
[sig-storage] Volume Placement should create and delete pod with multiple volumes from same datastore
[sig-storage] Volume Placement should create and delete pod with multiple volumes from different datastore
[sig-storage] Volume Operations Storm [Feature:vsphere] should create pod with many volumes and verify no attach call fails
[sig-storage] vsphere statefulset vsphere statefulset testing
[sig-service-catalog] [Feature:PodPreset] PodPreset should create a pod preset
[sig-service-catalog] [Feature:PodPreset] PodPreset should not modify the pod on conflict
[sig-storage] Detaching volumes should not work when mount is in progress
[sig-storage] Volume FStype [Feature:vsphere] verify fstype - ext3 formatted volume
[sig-storage] Volume limits should verify that all nodes have volume limits